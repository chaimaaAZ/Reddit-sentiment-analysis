{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw import Reddit\n",
    "from praw.models import Subreddit, Submission\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, date, timedelta\n",
    "from dotenv import dotenv_values\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords and stemmer for English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Preprocessing function for each comment\n",
    "def normalize_document(doc):\n",
    "    # Remove non-alphabetic characters (keep only letters)\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, flags=re.I | re.A)\n",
    "    doc = doc.lower().strip()\n",
    "    \n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Rebuild the document from the processed tokens\n",
    "    doc = ' '.join(tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying NLP on comments column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Dell\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='C:\\\\Users\\\\Dell\\\\nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_body</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Itâ€™s the country i live in and itâ€™s natural fo...</td>\n",
       "      <td>countri live natur compar leader want compar u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://en.wikipedia.org/wiki/North_American_C...</td>\n",
       "      <td>httpsenwikipediaorgwikinorthamericanchargingst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good point.</td>\n",
       "      <td>good point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If they can stay alive long enough. Most of th...</td>\n",
       "      <td>stay aliv long enough brand burn ton cash pole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google software ðŸ¤£\\nIt won't fix the laggy sinc...</td>\n",
       "      <td>googl softwar wont fix laggi sinc gooogl cant ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_body  \\\n",
       "0  Itâ€™s the country i live in and itâ€™s natural fo...   \n",
       "1  https://en.wikipedia.org/wiki/North_American_C...   \n",
       "2                                   Very good point.   \n",
       "3  If they can stay alive long enough. Most of th...   \n",
       "4  Google software ðŸ¤£\\nIt won't fix the laggy sinc...   \n",
       "\n",
       "                                  processed_comments  \n",
       "0  countri live natur compar leader want compar u...  \n",
       "1  httpsenwikipediaorgwikinorthamericanchargingst...  \n",
       "2                                         good point  \n",
       "3  stay aliv long enough brand burn ton cash pole...  \n",
       "4  googl softwar wont fix laggi sinc gooogl cant ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "full_df =  pd.read_csv(\"full_dataset_updated.csv\")\n",
    "full_df = full_df.dropna(subset=['comment_body'])\n",
    "full_df['post_created'] = pd.to_datetime(full_df['post_created'], format='%Y-%m-%d %H:%M:%S')\n",
    "full_df = full_df[full_df[\"comment_body\"]!=\"[removed]\"]\n",
    "full_df = full_df[full_df[\"comment_body\"]!=\"[deleted]\"]\n",
    "full_df['processed_comments'] = full_df['comment_body'].apply(normalize_document)\n",
    "full_df[['comment_body', 'processed_comments']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Uber\", \"youtube\", \"meta\", \"apple\", \"nvidia\", \"microsoft\", \"amazon\", \"Saudi Arabian Oil Co\", \"intel\", \"tesla\", \"MARA Holdings\"]\n",
    "company_dfs = {company: full_df[full_df['company'] == company] for company in full_df['company'].unique()}\n",
    "for company_name in keywords:\n",
    "    company_df = company_dfs.get(company_name)\n",
    "    \n",
    "    globals()[company_name] = company_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_score</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_num_comments</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_body</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>company</th>\n",
       "      <th>processed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Google in Crisis</td>\n",
       "      <td>718</td>\n",
       "      <td>1b4d5mn</td>\n",
       "      <td>533</td>\n",
       "      <td>2024-03-02 02:30:14</td>\n",
       "      <td>https://www.bigtechnology.com/p/inside-the-cri...</td>\n",
       "      <td>kt1g4fi</td>\n",
       "      <td>Clearly... \\n\\nWaymo actually has a fully auto...</td>\n",
       "      <td>4</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>clear waymo actual fulli autonom ride share se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Google in Crisis</td>\n",
       "      <td>718</td>\n",
       "      <td>1b4d5mn</td>\n",
       "      <td>533</td>\n",
       "      <td>2024-03-02 02:30:14</td>\n",
       "      <td>https://www.bigtechnology.com/p/inside-the-cri...</td>\n",
       "      <td>kszx1da</td>\n",
       "      <td>ChatGPT 4 can search the web with the paid ver...</td>\n",
       "      <td>18</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>chatgpt search web paid version</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Google in Crisis</td>\n",
       "      <td>718</td>\n",
       "      <td>1b4d5mn</td>\n",
       "      <td>533</td>\n",
       "      <td>2024-03-02 02:30:14</td>\n",
       "      <td>https://www.bigtechnology.com/p/inside-the-cri...</td>\n",
       "      <td>kt06obu</td>\n",
       "      <td>&gt;Just remember all of ChatGPTâ€™s data is over 2...</td>\n",
       "      <td>4</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>rememb chatgpt data year old actual true reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Google in Crisis</td>\n",
       "      <td>718</td>\n",
       "      <td>1b4d5mn</td>\n",
       "      <td>533</td>\n",
       "      <td>2024-03-02 02:30:14</td>\n",
       "      <td>https://www.bigtechnology.com/p/inside-the-cri...</td>\n",
       "      <td>kszy5m0</td>\n",
       "      <td>No it isnâ€™t, GPT4â€™s  latest training data is f...</td>\n",
       "      <td>17</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>isnt gpts latest train data april brows web li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Google in Crisis</td>\n",
       "      <td>718</td>\n",
       "      <td>1b4d5mn</td>\n",
       "      <td>533</td>\n",
       "      <td>2024-03-02 02:30:14</td>\n",
       "      <td>https://www.bigtechnology.com/p/inside-the-cri...</td>\n",
       "      <td>kt06d1j</td>\n",
       "      <td>&gt;I've used Chat GPT sometimes, but damn it's b...</td>\n",
       "      <td>3</td>\n",
       "      <td>microsoft</td>\n",
       "      <td>ive use chat gpt sometim damn bad free paid ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           post_title  post_score  post_id  post_num_comments  \\\n",
       "168  Google in Crisis         718  1b4d5mn                533   \n",
       "169  Google in Crisis         718  1b4d5mn                533   \n",
       "170  Google in Crisis         718  1b4d5mn                533   \n",
       "171  Google in Crisis         718  1b4d5mn                533   \n",
       "172  Google in Crisis         718  1b4d5mn                533   \n",
       "\n",
       "           post_created                                          post_body  \\\n",
       "168 2024-03-02 02:30:14  https://www.bigtechnology.com/p/inside-the-cri...   \n",
       "169 2024-03-02 02:30:14  https://www.bigtechnology.com/p/inside-the-cri...   \n",
       "170 2024-03-02 02:30:14  https://www.bigtechnology.com/p/inside-the-cri...   \n",
       "171 2024-03-02 02:30:14  https://www.bigtechnology.com/p/inside-the-cri...   \n",
       "172 2024-03-02 02:30:14  https://www.bigtechnology.com/p/inside-the-cri...   \n",
       "\n",
       "    comment_id                                       comment_body  \\\n",
       "168    kt1g4fi  Clearly... \\n\\nWaymo actually has a fully auto...   \n",
       "169    kszx1da  ChatGPT 4 can search the web with the paid ver...   \n",
       "170    kt06obu  >Just remember all of ChatGPTâ€™s data is over 2...   \n",
       "171    kszy5m0  No it isnâ€™t, GPT4â€™s  latest training data is f...   \n",
       "172    kt06d1j  >I've used Chat GPT sometimes, but damn it's b...   \n",
       "\n",
       "     comment_score    company  \\\n",
       "168              4  microsoft   \n",
       "169             18  microsoft   \n",
       "170              4  microsoft   \n",
       "171             17  microsoft   \n",
       "172              3  microsoft   \n",
       "\n",
       "                                    processed_comments  \n",
       "168  clear waymo actual fulli autonom ride share se...  \n",
       "169                    chatgpt search web paid version  \n",
       "170  rememb chatgpt data year old actual true reall...  \n",
       "171  isnt gpts latest train data april brows web li...  \n",
       "172  ive use chat gpt sometim damn bad free paid ve...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['microsoft'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VADER (Better for Social Media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return scores['compound']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saudi Arabian Oil Co DataFrame is None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_28804\\2906521426.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n"
     ]
    }
   ],
   "source": [
    "for company_name in keywords: \n",
    "    if company_name in globals():\n",
    "        df = globals()[company_name]\n",
    "        if df is None:\n",
    "            print(f\"{company_name} DataFrame is None.\")\n",
    "            continue\n",
    "        df['sentiment_score'] = df['processed_comments'].apply(analyze_sentiment_vader)\n",
    "\n",
    "        # Classify sentiment\n",
    "        df['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
    "\n",
    "        #df[['processed_comments', 'sentiment_score', 'sentiment']].head()\n",
    "        globals()[company_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_comments</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>didnt say didnt tech compani definit counterin...</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>eu compani slight less vicious manag us busi c...</td>\n",
       "      <td>-0.6378</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>thank ask plutu sound vagu</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>read post</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>want offer subscript app store make easier use...</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    processed_comments  sentiment_score  \\\n",
       "728  didnt say didnt tech compani definit counterin...           0.5574   \n",
       "729  eu compani slight less vicious manag us busi c...          -0.6378   \n",
       "730                         thank ask plutu sound vagu           0.3612   \n",
       "731                                          read post           0.0000   \n",
       "732  want offer subscript app store make easier use...           0.6124   \n",
       "\n",
       "    sentiment  \n",
       "728  positive  \n",
       "729  negative  \n",
       "730  positive  \n",
       "731   neutral  \n",
       "732  positive  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['meta'][['processed_comments', 'sentiment_score', 'sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                          post_title  post_score  post_id  \\\n",
      "0  Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
      "1  Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
      "2  Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
      "3  Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
      "4  Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
      "\n",
      "   post_num_comments        post_created  \\\n",
      "0                175 2024-03-01 03:45:26   \n",
      "1                175 2024-03-01 03:45:26   \n",
      "2                175 2024-03-01 03:45:26   \n",
      "3                175 2024-03-01 03:45:26   \n",
      "4                175 2024-03-01 03:45:26   \n",
      "\n",
      "                                           post_body comment_id  \\\n",
      "0  As of Thursday, owners of Ford electric vehicl...    kswcta9   \n",
      "1  As of Thursday, owners of Ford electric vehicl...    ksufbu2   \n",
      "2  As of Thursday, owners of Ford electric vehicl...    ksv9blq   \n",
      "3  As of Thursday, owners of Ford electric vehicl...    ksts85g   \n",
      "4  As of Thursday, owners of Ford electric vehicl...    ksuj8j5   \n",
      "\n",
      "                                        comment_body  comment_score company  \\\n",
      "0  Itâ€™s the country i live in and itâ€™s natural fo...              1   tesla   \n",
      "1  https://en.wikipedia.org/wiki/North_American_C...              7   tesla   \n",
      "2                                   Very good point.              1   tesla   \n",
      "3  If they can stay alive long enough. Most of th...              6   tesla   \n",
      "4  Google software ðŸ¤£\\nIt won't fix the laggy sinc...              3   tesla   \n",
      "\n",
      "                                  processed_comments  \n",
      "0  countri live natur compar leader want compar u...  \n",
      "1  httpsenwikipediaorgwikinorthamericanchargingst...  \n",
      "2                                         good point  \n",
      "3  stay aliv long enough brand burn ton cash pole...  \n",
      "4  googl softwar wont fix laggi sinc gooogl cant ...  \n"
     ]
    }
   ],
   "source": [
    "print(type(full_df))\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Dell\\ENSIAS\\S5\\Network analysis\\Reddit-sentiment-analysis\\venv_name\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentiment'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Filter only positive comments\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m positive_comments \u001b[38;5;241m=\u001b[39m full_df[\u001b[43mfull_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Count the number of positive comments for each company\u001b[39;00m\n\u001b[0;32m      5\u001b[0m positive_comment_counts \u001b[38;5;241m=\u001b[39m positive_comments[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\ENSIAS\\S5\\Network analysis\\Reddit-sentiment-analysis\\venv_name\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Dell\\ENSIAS\\S5\\Network analysis\\Reddit-sentiment-analysis\\venv_name\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentiment'"
     ]
    }
   ],
   "source": [
    "# Filter only positive comments\n",
    "positive_comments = full_df[full_df['sentiment'] == 'positive']\n",
    "\n",
    "# Count the number of positive comments for each company\n",
    "positive_comment_counts = positive_comments['company'].value_counts()\n",
    "\n",
    "# Get the company with the highest count\n",
    "top_company = positive_comment_counts.idxmax()\n",
    "top_count = positive_comment_counts.max()\n",
    "\n",
    "print(f\"The company with the highest number of positive comments is {top_company} with {top_count} positive comments.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
