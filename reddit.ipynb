{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from praw import Reddit\n",
    "from praw.models import Subreddit, Submission\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, date, timedelta\n",
    "from dotenv import dotenv_values\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from datetime import datetime\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config[\"CLIENT_ID\"],\n",
    "    client_secret=config[\"CLIENT_SECRET\"],\n",
    "    redirect_uri=config[\"REDIRECT_URL\"],\n",
    "    user_agent=config[\"USER_AGENT\"]\n",
    ")\n",
    "\n",
    "def fetch_posts_about_keyword(subreddit_name, keywords, limit=500000):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    data = []\n",
    "    \n",
    "    # Define the cutoff date (before November 2024)\n",
    "    cutoff_date = datetime(2024, 11, 30)  # November 1, 2024\n",
    "    \n",
    "    # Fetch posts from subreddit\n",
    "    for post in subreddit.search(' '.join(keywords), time_filter='all', limit=limit):\n",
    "        post_date = datetime.utcfromtimestamp(post.created)\n",
    "        \n",
    "        # Check if the post is before the cutoff date\n",
    "        if post_date < cutoff_date:\n",
    "            matching_keywords = [keyword for keyword in keywords if keyword.lower() in post.title.lower()]\n",
    "\n",
    "            for keyword in matching_keywords:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                comments = post.comments.list()\n",
    "\n",
    "                for comment in comments:\n",
    "                    data.append({\n",
    "                        'post_title': post.title,\n",
    "                        'post_score': post.score,\n",
    "                        'post_id': post.id,\n",
    "                        'post_num_comments': post.num_comments,\n",
    "                        'post_created': post_date,\n",
    "                        'post_body': post.selftext,\n",
    "                        'comment_id': comment.id,\n",
    "                        'comment_body': comment.body,\n",
    "                        'comment_score': comment.score,\n",
    "                        'matched_keywords': \", \".join(matching_keywords)\n",
    "                    })\n",
    "                break  # Only process the post once per matching keyword\n",
    "\n",
    "    return data\n",
    "\n",
    "subreddit_name = \"stocks\"\n",
    "keywords = [\"Uber\", \"youtube\", \"meta\", \"apple\", \"nvidia\", \"microsoft\", \"amazon\", \"Saudi Arabian Oil Co\", \"intel\", \"tesla\", \"MARA Holdings\"]\n",
    "\n",
    "# Fetch posts before November 2024\n",
    "data = fetch_posts_about_keyword(subreddit_name, keywords, limit=1000)\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"posts_before_november_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts for keyword: Uber\n",
      "Fetching posts for keyword: youtube\n",
      "Fetching posts for keyword: meta\n",
      "Fetching posts for keyword: apple\n",
      "Fetching posts for keyword: nvidia\n",
      "Fetching posts for keyword: microsoft\n",
      "Fetching posts for keyword: amazon\n",
      "Fetching posts for keyword: Saudi Arabian Oil Co\n",
      "Fetching posts for keyword: intel\n",
      "Fetching posts for keyword: tesla\n",
      "Fetching posts for keyword: MARA Holdings\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config[\"CLIENT_ID\"],\n",
    "    client_secret=config[\"CLIENT_SECRET\"],\n",
    "    user_agent=config[\"USER_AGENT\"]\n",
    ")\n",
    "\n",
    "def fetch_posts_about_keywords(subreddit_name, keywords, limit=500000):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    data = []\n",
    "    \n",
    "    # Define the date range for March and April (2024)\n",
    "    today = datetime(2024, 9, 30, 23, 59, 59)  # End of April\n",
    "    start_date = datetime(2024, 7, 1)  # Start of March\n",
    "    \n",
    "    # Iterate through each keyword\n",
    "    for keyword in keywords:\n",
    "        print(f\"Fetching posts for keyword: {keyword}\")\n",
    "        for post in subreddit.search(keyword, time_filter='all', limit=limit):\n",
    "            post_date = datetime.utcfromtimestamp(post.created)\n",
    "            \n",
    "            # Check if the post is within the desired time range\n",
    "            if start_date <= post_date <= today:\n",
    "                post.comments.replace_more(limit=0)\n",
    "                comments = post.comments.list()\n",
    "\n",
    "                for comment in comments:\n",
    "                    data.append({\n",
    "                        'post_title': post.title,\n",
    "                        'post_score': post.score,\n",
    "                        'post_id': post.id,\n",
    "                        'post_num_comments': post.num_comments,\n",
    "                        'post_created': post_date,\n",
    "                        'post_body': post.selftext,\n",
    "                        'comment_id': comment.id,\n",
    "                        'comment_body': comment.body,\n",
    "                        'comment_score': comment.score,\n",
    "                        'matched_keyword': keyword\n",
    "                    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "subreddit_name = \"stocks\"\n",
    "keywords = [\"Uber\", \"youtube\", \"meta\", \"apple\", \"nvidia\", \"microsoft\", \"amazon\", \"Saudi Arabian Oil Co\", \"intel\", \"tesla\", \"MARA Holdings\"]\n",
    "\n",
    "# Fetch posts from March and April 2024\n",
    "data = fetch_posts_about_keywords(subreddit_name, keywords, limit=500000)\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"before_september_reddit_comments.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaimaa\\AppData\\Local\\Temp\\ipykernel_26072\\1616194821.py:1: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  full_df = pd.read_csv(\"full_dataset.csv\")\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"full_dataset.csv\")\n",
    "full_df['company'] = full_df['matched_keyword'].fillna(full_df['matched_keywords'])\n",
    "full_df.drop(columns=['matched_keyword', 'matched_keywords'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_score</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_num_comments</th>\n",
       "      <th>post_created</th>\n",
       "      <th>post_body</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>comment_score</th>\n",
       "      <th>company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tesla stands to earn billions by opening U.S. ...</td>\n",
       "      <td>548</td>\n",
       "      <td>1b3kl34</td>\n",
       "      <td>175</td>\n",
       "      <td>2024-03-01 03:45:26</td>\n",
       "      <td>As of Thursday, owners of Ford electric vehicl...</td>\n",
       "      <td>kswcta9</td>\n",
       "      <td>It’s the country i live in and it’s natural fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla stands to earn billions by opening U.S. ...</td>\n",
       "      <td>548</td>\n",
       "      <td>1b3kl34</td>\n",
       "      <td>175</td>\n",
       "      <td>2024-03-01 03:45:26</td>\n",
       "      <td>As of Thursday, owners of Ford electric vehicl...</td>\n",
       "      <td>ksufbu2</td>\n",
       "      <td>https://en.wikipedia.org/wiki/North_American_C...</td>\n",
       "      <td>7</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tesla stands to earn billions by opening U.S. ...</td>\n",
       "      <td>548</td>\n",
       "      <td>1b3kl34</td>\n",
       "      <td>175</td>\n",
       "      <td>2024-03-01 03:45:26</td>\n",
       "      <td>As of Thursday, owners of Ford electric vehicl...</td>\n",
       "      <td>ksv9blq</td>\n",
       "      <td>Very good point.</td>\n",
       "      <td>1</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla stands to earn billions by opening U.S. ...</td>\n",
       "      <td>548</td>\n",
       "      <td>1b3kl34</td>\n",
       "      <td>175</td>\n",
       "      <td>2024-03-01 03:45:26</td>\n",
       "      <td>As of Thursday, owners of Ford electric vehicl...</td>\n",
       "      <td>ksts85g</td>\n",
       "      <td>If they can stay alive long enough. Most of th...</td>\n",
       "      <td>6</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tesla stands to earn billions by opening U.S. ...</td>\n",
       "      <td>548</td>\n",
       "      <td>1b3kl34</td>\n",
       "      <td>175</td>\n",
       "      <td>2024-03-01 03:45:26</td>\n",
       "      <td>As of Thursday, owners of Ford electric vehicl...</td>\n",
       "      <td>ksuj8j5</td>\n",
       "      <td>Google software 🤣\\nIt won't fix the laggy sinc...</td>\n",
       "      <td>3</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70245</th>\n",
       "      <td>Why I think UBER is a BUY</td>\n",
       "      <td>0</td>\n",
       "      <td>1h2kshj</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-11-29 12:32:18</td>\n",
       "      <td>**Personal trading context:**\\n\\nI do not look...</td>\n",
       "      <td>lzjzffi</td>\n",
       "      <td>Who can make self-driving network and operate ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Uber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70246</th>\n",
       "      <td>Why I think UBER is a BUY</td>\n",
       "      <td>0</td>\n",
       "      <td>1h2kshj</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-11-29 12:32:18</td>\n",
       "      <td>**Personal trading context:**\\n\\nI do not look...</td>\n",
       "      <td>lzn41d8</td>\n",
       "      <td>Relax, he is a \"Joseph Carlson\" bot. Username ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Uber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70247</th>\n",
       "      <td>Why I think UBER is a BUY</td>\n",
       "      <td>0</td>\n",
       "      <td>1h2kshj</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-11-29 12:32:18</td>\n",
       "      <td>**Personal trading context:**\\n\\nI do not look...</td>\n",
       "      <td>lzl059p</td>\n",
       "      <td>&gt; Uber might, but Tesla also might not allow t...</td>\n",
       "      <td>2</td>\n",
       "      <td>Uber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70248</th>\n",
       "      <td>Why I think UBER is a BUY</td>\n",
       "      <td>0</td>\n",
       "      <td>1h2kshj</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-11-29 12:32:18</td>\n",
       "      <td>**Personal trading context:**\\n\\nI do not look...</td>\n",
       "      <td>lzjyx6d</td>\n",
       "      <td>I have for 3 I'll let u know in another 2. not...</td>\n",
       "      <td>0</td>\n",
       "      <td>Uber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70249</th>\n",
       "      <td>Why I think UBER is a BUY</td>\n",
       "      <td>0</td>\n",
       "      <td>1h2kshj</td>\n",
       "      <td>52</td>\n",
       "      <td>2024-11-29 12:32:18</td>\n",
       "      <td>**Personal trading context:**\\n\\nI do not look...</td>\n",
       "      <td>lzjypqj</td>\n",
       "      <td>The topic is you thinking that beating the mar...</td>\n",
       "      <td>1</td>\n",
       "      <td>Uber, meta, tesla</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70250 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              post_title  post_score  post_id   \n",
       "0      Tesla stands to earn billions by opening U.S. ...         548  1b3kl34  \\\n",
       "1      Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
       "2      Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
       "3      Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
       "4      Tesla stands to earn billions by opening U.S. ...         548  1b3kl34   \n",
       "...                                                  ...         ...      ...   \n",
       "70245                          Why I think UBER is a BUY           0  1h2kshj   \n",
       "70246                          Why I think UBER is a BUY           0  1h2kshj   \n",
       "70247                          Why I think UBER is a BUY           0  1h2kshj   \n",
       "70248                          Why I think UBER is a BUY           0  1h2kshj   \n",
       "70249                          Why I think UBER is a BUY           0  1h2kshj   \n",
       "\n",
       "       post_num_comments         post_created   \n",
       "0                    175  2024-03-01 03:45:26  \\\n",
       "1                    175  2024-03-01 03:45:26   \n",
       "2                    175  2024-03-01 03:45:26   \n",
       "3                    175  2024-03-01 03:45:26   \n",
       "4                    175  2024-03-01 03:45:26   \n",
       "...                  ...                  ...   \n",
       "70245                 52  2024-11-29 12:32:18   \n",
       "70246                 52  2024-11-29 12:32:18   \n",
       "70247                 52  2024-11-29 12:32:18   \n",
       "70248                 52  2024-11-29 12:32:18   \n",
       "70249                 52  2024-11-29 12:32:18   \n",
       "\n",
       "                                               post_body comment_id   \n",
       "0      As of Thursday, owners of Ford electric vehicl...    kswcta9  \\\n",
       "1      As of Thursday, owners of Ford electric vehicl...    ksufbu2   \n",
       "2      As of Thursday, owners of Ford electric vehicl...    ksv9blq   \n",
       "3      As of Thursday, owners of Ford electric vehicl...    ksts85g   \n",
       "4      As of Thursday, owners of Ford electric vehicl...    ksuj8j5   \n",
       "...                                                  ...        ...   \n",
       "70245  **Personal trading context:**\\n\\nI do not look...    lzjzffi   \n",
       "70246  **Personal trading context:**\\n\\nI do not look...    lzn41d8   \n",
       "70247  **Personal trading context:**\\n\\nI do not look...    lzl059p   \n",
       "70248  **Personal trading context:**\\n\\nI do not look...    lzjyx6d   \n",
       "70249  **Personal trading context:**\\n\\nI do not look...    lzjypqj   \n",
       "\n",
       "                                            comment_body  comment_score   \n",
       "0      It’s the country i live in and it’s natural fo...              1  \\\n",
       "1      https://en.wikipedia.org/wiki/North_American_C...              7   \n",
       "2                                       Very good point.              1   \n",
       "3      If they can stay alive long enough. Most of th...              6   \n",
       "4      Google software 🤣\\nIt won't fix the laggy sinc...              3   \n",
       "...                                                  ...            ...   \n",
       "70245  Who can make self-driving network and operate ...              1   \n",
       "70246  Relax, he is a \"Joseph Carlson\" bot. Username ...              1   \n",
       "70247  > Uber might, but Tesla also might not allow t...              2   \n",
       "70248  I have for 3 I'll let u know in another 2. not...              0   \n",
       "70249  The topic is you thinking that beating the mar...              1   \n",
       "\n",
       "                 company  \n",
       "0                  tesla  \n",
       "1                  tesla  \n",
       "2                  tesla  \n",
       "3                  tesla  \n",
       "4                  tesla  \n",
       "...                  ...  \n",
       "70245               Uber  \n",
       "70246               Uber  \n",
       "70247               Uber  \n",
       "70248               Uber  \n",
       "70249  Uber, meta, tesla  \n",
       "\n",
       "[70250 rows x 10 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame for tesla created.\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"Uber\", \"youtube\", \"meta\", \"apple\", \"nvidia\", \"microsoft\", \"amazon\", \"Saudi Arabian Oil Co\", \"intel\", \"tesla\", \"MARA Holdings\"]\n",
    "company_dfs = {company: full_df[full_df['company'] == company] for company in full_df['company'].unique()}\n",
    "for company_name in keywords:\n",
    "    company_df = company_dfs.get(company_name)\n",
    "    \n",
    "    globals()[company_name] = company_df\n",
    "    \n",
    "print(f\"DataFrame for tesla created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of comments column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
